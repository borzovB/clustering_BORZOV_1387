import pylab as plt
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import LabelEncoder

print("НАЧАЛО РАБОТЫ!")
#%%
print("Загрузка датасета")
# Загрузка файла
df = pd.read_csv("data/pdb_data_no_dups.csv")
#%%
print(f"Число строк и столбцов в датасете: {df.shape}")
#%%
print("Типы переменных.")
df.dtypes
#%%
print("Полная статистика для всех столбцов.")
print(df.describe(include="all"))

#%%
print("Пример заполнения данных")
print(df.head())
#%%
print("Доля пустых переменных для каждой переменной")
columns_to_check = [
    'structureId', 'classification', 'experimentalTechnique',
    'macromoleculeType', 'residueCount', 'resolution',
    'structureMolecularWeight', 'crystallizationMethod',
    'crystallizationTempK', 'densityMatthews',
    'densityPercentSol', 'pdbxDetails', 'phValue', 'publicationYear'
]

for col in columns_to_check:
    if col in df.columns:
        count = df[col].isnull().sum()
        pct = (count / len(df)) * 100
        print(f"{col:<25} → {pct:6.2f}%")
#%%
print("Удаление лишних столбцов")
df=df.drop(['publicationYear', 'structureId','pdbxDetails'], axis = 1)

print("\nОпределение категориальных и числовых переменных")
numeric_cols = df.select_dtypes(include='number').columns
categorical_cols = df.select_dtypes(include='object').columns

print(f"Числовые переменные: {numeric_cols}")
print(f"Категориальные переменные: {categorical_cols}")
#%%
# Кодируем категориальные признаки только для определения зависимостей между переменными
df_new = df.copy()

df_new['experimentalTechnique']   = LabelEncoder().fit_transform(df_new['experimentalTechnique'].astype(str))
df_new['crystallizationMethod']  = LabelEncoder().fit_transform(df_new['crystallizationMethod'].astype(str))
df_new['classification']  = LabelEncoder().fit_transform(df_new['classification'].astype(str))
df_new['macromoleculeType']  = LabelEncoder().fit_transform(df_new['macromoleculeType'].astype(str))
df_new['densityMatthews']  = LabelEncoder().fit_transform(df_new['densityMatthews'].astype(str))

# Смотрим корреляцию
print("Матрица корреляций всех признаков, которые пойдут в кластеризацию")
plt.figure(figsize=(11,9))
sns.heatmap(df_new.corr(), annot=True, cmap='coolwarm', center=0, fmt='.2f', linewidths=0.5, square=True)
plt.title("Корреляция Пирсона всех 8 признаков", fontsize=14)
plt.show()
#%%
print("Удаление лишних столбцов")
df_new=df_new.drop(['macromoleculeType', 'densityMatthews'], axis = 1)
#%%
df_new.head()
#%%
print("\nДиаграмма рассеивания")

sns.pairplot(df_new, plot_kws={'alpha': 0.7})
plt.suptitle("Pairplot всех попарных диаграмм рассеяния", y=1.02)
plt.show()
#%%
print("Удаление лишних столбцов")
df=df.drop(['macromoleculeType', 'densityMatthews', 'residueCount'], axis = 1)
#%%

numeric_cols = df.select_dtypes(include='number').columns.tolist()

# Масштабирования диапазона на графике
scales = {
    'phValue': (0, 14),
    'crystallizationTempK': (250, 350),
    'resolution': (0, 10),
    'densityPercentSol': (20, 80),
    'residueCount': (0, 3000),
    'structureMolecularWeight': (0, 1e6),
}

fig, axes = plt.subplots(3, 3, figsize=(16, 12))
axes = axes.ravel()
fig.suptitle("Гистограммы числовых признаков (с физическим масштабом)", fontsize=16, y=0.98)

for idx, col in enumerate(numeric_cols):
    ax = axes[idx]
    range_val = scales.get(col, (None, None))
    df_new[col].hist(bins=50, ax=ax, range=range_val, edgecolor='black', color='lightblue')
    ax.set_title(col)
    ax.set_xlabel(col)
    ax.set_ylabel("Количество")
    ax.grid(False)

for idx in range(len(numeric_cols), 9):
    axes[idx].set_visible(False)

plt.tight_layout()
plt.show()

# Ящики с усами
for col in numeric_cols:
    plt.figure(figsize=(8, 4))
    df[[col]].boxplot()
    plt.title(f"Выбросы: {col}")
    plt.show()

# Гистограммы категориальных переменных по классам
for col in categorical_cols:
    if col in df.columns:
        plt.figure(figsize=(10, 5))
        top10 = df[col].value_counts().head(10)
        top10.plot(kind='bar')
        plt.title(f"Топ-10 значений: {col}")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
#%%
print(f"Количество дублирующих строк: {df.duplicated().sum()}")
#%%
print(f"Число строк и столбцов в датасете: {df.shape}")
#%%
print("Удаление дубликатов.")
df = df.drop_duplicates().reset_index(drop=True)
print(f"Количество дублирующих строк: {df.duplicated().sum()}")
#%%
print(f"Число строк и столбцов в датасете: {df.shape}")
#%%
df.isnull().sum()
#%%
print("Заполняем пустые значения для строк модой")
for col in df.select_dtypes(include=['object', 'category']).columns:
    df[col] = df[col].fillna(df[col].mode()[0])
#%%
df.isnull().sum()
#%%
# Линейная регрессия будет применяться для генерации значений
from LinearRegression import LinearRegression
print("Начать починку!")
df_fild = df.copy() # Копируем таблицу, чтобы избежать ненужного изменения данных
number = df.select_dtypes(include='number').columns.tolist() # Получение наименований числовых столбцов из датасета

for n in range(5): # Проводим 5 итераций, это позволяет лучше нормализовать данные, чтобы при заполнении данными пустых ячеек
    # не сгенерировать значения увеличивающие разбалансировку, например, чтобы между числами не было сильного разрыва
    for column in number:
        # Начало генерации данных для каждого столбца
        df_no_null = df_fild[df_fild[column].notnull()].copy() # Выявление строк, где нет пустых значений, они понадобятся для обучения модели это хорошие данные необходимы для обучения модели
        df_null = df_fild[df_fild[column].isnull()].copy() # Выявление строк с пустыми ячейками, прохие данные

        if df_null.empty:
            # Если в столбце нет пустых ячеек, то пропускаем его обработку
            continue

        fact_column = [col for col in number if col!=column] # Подготавливаем данные выявляем все столбцы кроме обрабатываемого, они будут необходимы для обучения модели
        X = df_no_null[fact_column].copy() # Данные из хороших строк, необходимы для обучения модели
        for nice in fact_column:
            X[nice] = X[nice].fillna(X[nice].mean())
            # Заполняем данные для обучения средними значениями

        y = df_no_null[column].values
        # Целевая переменная без NaN


        X_try = df_null[fact_column].copy() # Данные из плохих строк, необходимы
        # для дальнейшего заполнения
        for bed in fact_column:
            X_try[bed] = X_try[bed].fillna(X[bed].mean()) # Временно заполняем средними значениями взятые из хороших данных

        mean = np.mean(X, axis=0) # Среднее значение по столбцам
        std = np.std(X, axis=0) # Разброс значений по столбцам
        std[std == 0] = 1 # Если разброс значений 0, то необходимо заполнить пустые значений 1, чтобы нормализация прошла

        X = (X-mean)/std
        X_try = (X_try-mean)/std

        li = LinearRegression()
        li.fit(X,y)
        predicted_values = li.predict(X_try)

        df_fild.loc[df_fild[column].isnull(), column] = predicted_values # Записываем угаданные значения в пустые клетки
        df = df_fild
#%%
df.isnull().sum()
#%%
numeric_cols = df.select_dtypes(include='number').columns
df_model = df.copy()

for col in numeric_cols:
    Q1 = df_model[col].quantile(0.25)
    Q3 = df_model[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_model = df_model[(df_model[col] >= lower_bound) & (df_model[col] <= upper_bound)]

print(f"Количество строк после удаления выбросов: {len(df_model)}")
#%%
print("ЯЩИК С УСАМИ ПОСЛЕ ОБРАБОТКИ ")
for col in numeric_cols:
    plt.figure(figsize=(8, 4))
    df_model[[col]].boxplot()
    plt.title(f"Выбросы: {col}")
    plt.show()
#%%
from sklearn.model_selection import train_test_split

# Уменьшение данных с сохранением пропорций
desired_total = 25_000

print(f"Исходный размер: {len(df_model):,} строк")
print(f"Уникальных классов: {df_model['classification'].nunique()}")

# 1. Убираем классы, где меньше 2 объектов (иначе stratify упадёт)
class_counts = df_model['classification'].value_counts()
valid_classes = class_counts[class_counts >= 2].index
df_filtered = df_model[df_model['classification'].isin(valid_classes)]

print(f"После удаления одиночных классов: {len(df_filtered):,} строк")
print(f"Классов осталось: {df_filtered['classification'].nunique()}")

# 2. Теперь можно делать стратифицированную подвыборку
fraction = desired_total / len(df_filtered)
if fraction > 1:
    fraction = 1.0  # если и так меньше — берём всё

df_sampled, _ = train_test_split(
    df_filtered,
    test_size=1 - fraction,
    random_state=42,
    stratify=df_filtered['classification']
)

df_sampled = df_sampled.reset_index(drop=True)

print(f"\nПодвыборка: {len(df_sampled):,} строк")
print(f"Классов в подвыборке: {df_sampled['classification'].nunique()}")
#%%
print("БЛОК подготовки данных для кластеризации С КЛЮЧЕВЫМИ ПРИЗНАКАМИ")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from imblearn.over_sampling import RandomOverSampler

# 1. Берем только TOP-N классов для кластеризации
print("ВЫБОР TOP-N КЛАССОВ ДЛЯ КЛАСТЕРИЗАЦИИ")
top_n_classes = 15
class_counts = df_sampled['classification'].value_counts()
top_classes = class_counts.head(top_n_classes).index

print(f"Всего классов в df_sampled: {df_sampled['classification'].nunique()}")
print(f"Берем топ-{top_n_classes} классов для кластеризации:")

# Создаем датасет только с топ-классами
df_cluster = df_sampled[df_sampled['classification'].isin(top_classes)].copy()
df_cluster = df_cluster.reset_index(drop=True)

print(f"Размер датасета для кластеризации: {len(df_cluster):,} строк")
print(f"Классов в df_cluster: {df_cluster['classification'].nunique()}")
print("\nТоп-15 классов:")
for i, (cls, count) in enumerate(class_counts.head(top_n_classes).items()):
    print(f"  {i+1:2d}. {cls:<30} - {count:>4} образцов")

# 2. БАЛАНСИРОВКА ДАННЫХ С ПОМОЩЬЮ ROS (Random OverSampling)
print("\nБАЛАНСИРОВКА ДАННЫХ МЕТОДОМ ROS")

# Сначала подготавливаем данные без балансировки для ROS
X_temp = df_cluster.drop(columns=['classification'])
y_temp = df_cluster['classification']

# Кодируем категориальные переменные для балансировки
label_encoders_temp = {}
for col in X_temp.select_dtypes(include='object').columns:
    le = LabelEncoder()
    X_temp[col] = le.fit_transform(X_temp[col].astype(str))
    label_encoders_temp[col] = le

# Применяем Random OverSampling
ros = RandomOverSampler(random_state=42)
X_balanced, y_balanced = ros.fit_resample(X_temp, y_temp)

print(f"До балансировки: {len(X_temp):,} образцов")
print(f"После балансировки ROS: {len(X_balanced):,} образцов")

# Создаем сбалансированный датасет
df_balanced = pd.DataFrame(X_balanced, columns=X_temp.columns)
df_balanced['classification'] = y_balanced

# Анализ распределения после балансировки
print("\nРАСПРЕДЕЛЕНИЕ ПОСЛЕ БАЛАНСИРОВКИ:")
balanced_counts = df_balanced['classification'].value_counts()
for i, (cls, count) in enumerate(balanced_counts.items()):
    print(f"  {i+1:2d}. {cls:<30} - {count:>4} образцов")

print(f"Все классы содержат по {balanced_counts.iloc[0]} образцов")

# 3. ВЫБОР КЛЮЧЕВЫХ ПРИЗНАКОВ ДЛЯ КЛАСТЕРИЗАЦИИ
print("\nВЫБОР КЛЮЧЕВЫХ ПРИЗНАКОВ ДЛЯ КЛАСТЕРИЗАЦИИ")

# На основе анализа PCA выбираем самые важные признаки
key_features = ['resolution', 'densityPercentSol', 'structureMolecularWeight', 'experimentalTechnique']

print("Ключевые признаки для кластеризации:")
for i, feature in enumerate(key_features, 1):
    print(f"  {i}. {feature}")

# 4. Подготовка данных для кластеризации (только ключевые признаки)
print("\nПОДГОТОВКА ПРИЗНАКОВ НА СБАЛАНСИРОВАННЫХ ДАННЫХ")
df_cluster = df_balanced  # Используем сбалансированные данные

# Берем только ключевые признаки
X_cluster = df_cluster[key_features].copy()

# Кодируем категориальные переменные (experimentalTechnique)
label_encoders = {}
for col in X_cluster.select_dtypes(include='object').columns:
    le = LabelEncoder()
    X_cluster[col] = le.fit_transform(X_cluster[col].astype(str))
    label_encoders[col] = le

# Нормализация
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cluster)

# Реальные метки для сравнения (закодированные)
real_labels = LabelEncoder().fit_transform(df_cluster['classification'])

print(f"\nДАННЫЕ ПОДГОТОВЛЕНЫ И СБАЛАНСИРОВАНЫ!")
print(f"Форма данных для кластеризации: {X_scaled.shape}")
print(f"Размер real_labels: {real_labels.shape}")
print(f"Количество уникальных реальных классов: {len(np.unique(real_labels))}")
print(f"Используемые признаки: {key_features}")

# 5. Анализ распределения классов после балансировки
print(f"\nРАСПРЕДЕЛЕНИЕ КЛАССОВ ПОСЛЕ БАЛАНСИРОВКИ:")
unique, counts = np.unique(real_labels, return_counts=True)
for cls_idx, count in zip(unique, counts):
    original_class = df_cluster['classification'].iloc[np.where(real_labels == cls_idx)[0][0]]
    print(f"  Класс {cls_idx:2d} ({original_class:<30}): {count:>4} образцов")

# Проверка балансировки
if len(np.unique(counts)) == 1:
    print("ИДЕАЛЬНАЯ БАЛАНСИРОВКА! Все классы имеют одинаковое количество образцов")
else:
    print(f"Небольшой разброс: от {counts.min()} до {counts.max()} образцов")

# 6. Создаем PCA для визуализации
print("\nPCA ДЛЯ ВИЗУАЛИЗАЦИИ")
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
print(f"Объясненная дисперсия PCA: {pca.explained_variance_ratio_.sum():.3f}")

# Анализ нагрузки признаков на новые компоненты
print("\nНАГРУЗКА ПРИЗНАКОВ НА КОМПОНЕНТЫ:")
for i, component in enumerate(pca.components_):
    print(f"PC{i+1}: {dict(zip(key_features, np.abs(component).round(3)))}")

# Визуализация распределения классов в PCA
plt.figure(figsize=(12, 5))

# До балансировки (только ключевые признаки)
plt.subplot(1, 2, 1)
X_temp_key = X_temp[key_features].copy()
for col in X_temp_key.select_dtypes(include='object').columns:
    X_temp_key[col] = label_encoders_temp[col].transform(X_temp_key[col].astype(str))
X_temp_scaled = scaler.fit_transform(X_temp_key)
X_temp_pca = PCA(n_components=2).fit_transform(X_temp_scaled)
temp_labels = LabelEncoder().fit_transform(y_temp)
plt.scatter(X_temp_pca[:, 0], X_temp_pca[:, 1], c=temp_labels, cmap='tab10', alpha=0.6, s=10)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('ДО балансировки\n(сильный дисбаланс)')

# После балансировки
plt.subplot(1, 2, 2)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=real_labels, cmap='tab10', alpha=0.6, s=10)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
plt.title('ПОСЛЕ балансировки ROS\n(4 ключевых признака)')
plt.colorbar(scatter, label='Классы')

plt.tight_layout()
plt.show()

print(f"\nГОТОВО К КЛАСТЕРИЗАЦИИ НА СБАЛАНСИРОВАННЫХ ДАННЫХ С КЛЮЧЕВЫМИ ПРИЗНАКАМИ!")
print(f"Используемые признаки: {key_features}")
print(f"Размер данных: {X_scaled.shape}")
#%%
print(X_cluster.head())
#%%
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering

print("ЗАПУСК КЛАСТЕРИЗАЦИИ")
#%%
print("\nБЛОК 2: K-MEANS - ОПРЕДЕЛЕНИЕ ОПТИМАЛЬНОГО K")

import time
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import numpy as np
import matplotlib.pyplot as plt

# ОГРАНИЧИВАЕМ РАЗМЕР ДЛЯ БЫСТРОГО ПОДБОРА K
max_samples_for_k_selection = 5000

if len(X_scaled) > max_samples_for_k_selection:
    indices = np.random.choice(len(X_scaled), max_samples_for_k_selection, replace=False)
    X_fast = X_scaled[indices]
    print(f"Для подбора k используем подвыборку: {len(X_fast)} образцов")
else:
    X_fast = X_scaled
    print(f"Используем все данные: {len(X_fast)} образцов")

def evaluate_kmeans_fast(X, k_range, n_init=5, random_state=42):
    inertias = []
    silhouettes = []
    times = []

    for k in k_range:
        start_time = time.time()
        kmeans = KMeans(n_clusters=k, n_init=n_init, random_state=random_state)
        labels = kmeans.fit_predict(X)
        end_time = time.time()

        inertias.append(kmeans.inertia_)
        silhouettes.append(silhouette_score(X, labels))
        times.append(end_time - start_time)

        print(f"  k={k} завершен за {times[-1]:.2f} сек")

    return inertias, silhouettes, times

k_range = range(2, 11)
print(f"Тестируем k в диапазоне: {list(k_range)}")

inertias, silhouettes, times_kmeans = evaluate_kmeans_fast(X_fast, k_range)

# Визуализация
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)
axes[0].set_xlabel('Количество кластеров')
axes[0].set_ylabel('Инерция')
axes[0].set_title('Метод локтя для K-Means')
axes[0].grid(True, alpha=0.3)

axes[1].plot(k_range, silhouettes, 'ro-', linewidth=2, markersize=8)
axes[1].set_xlabel('Количество кластеров')
axes[1].set_ylabel('Силуэтный коэффициент')
axes[1].set_title('Силуэтный анализ для K-Means')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ОПРЕДЕЛЯЕМ OPTIMAL_K
optimal_k_silhouette = k_range[np.argmax(silhouettes)]
optimal_k = optimal_k_silhouette  # СОЗДАЕМ ПЕРЕМЕННУЮ optimal_k

print(f"Оптимальное k (силуэтный анализ): {optimal_k}")
print(f"Силуэтный коэффициент: {max(silhouettes):.3f}")

# ФИНАЛЬНОЕ ОБУЧЕНИЕ
print(f"\nФИНАЛЬНОЕ ОБУЧЕНИЕ K-MEANS НА ВСЕХ ДАННЫХ...")
start_time = time.time()
kmeans_final = KMeans(n_clusters=optimal_k, n_init=10, random_state=42)
kmeans_labels = kmeans_final.fit_predict(X_scaled)
final_time = time.time() - start_time

print(f"K-Means обучен на {len(X_scaled):,} образцах за {final_time:.2f} сек")
print(f"Найдено кластеров: {len(np.unique(kmeans_labels))}")
#%%
print("\nБЛОК 3: K-MEANS - МЕТОДЫ ИНИЦИАЛИЗАЦИИ")

from sklearn.metrics import adjusted_rand_score
import time

# ОГРАНИЧИВАЕМ ДАННЫЕ ДЛЯ СРАВНЕНИЯ МЕТОДОВ
max_samples_comparison = 10000

if len(X_scaled) > max_samples_comparison:
    indices = np.random.choice(len(X_scaled), max_samples_comparison, replace=False)
    X_fast_comparison = X_scaled[indices]
    real_labels_fast = real_labels[indices]
    print(f"Для сравнения методов используем подвыборку: {len(X_fast_comparison)} образцов")
else:
    X_fast_comparison = X_scaled
    real_labels_fast = real_labels
    print(f"Используем все данные: {len(X_fast_comparison)} образцов")

init_methods = ['k-means++', 'random']
n_runs = 3
results_init = []

print("Сравнение методов инициализации...")

for init_method in init_methods:
    print(f"\nТестирование метода: {init_method}")

    for run in range(n_runs):
        start_time = time.time()
        kmeans = KMeans(n_clusters=optimal_k, init=init_method, n_init=1, random_state=run)
        labels = kmeans.fit_predict(X_fast_comparison)
        end_time = time.time()

        results_init.append({
            'method': init_method,
            'run': run,
            'inertia': kmeans.inertia_,
            'silhouette': silhouette_score(X_fast_comparison, labels),
            'time': end_time - start_time,
            'ari': adjusted_rand_score(real_labels_fast, labels)
        })

        print(f"  Запуск {run+1}/{n_runs} завершен за {end_time - start_time:.2f} сек")

results_init_df = pd.DataFrame(results_init)

print("\n" + "="*50)
print("СРАВНЕНИЕ МЕТОДОВ ИНИЦИАЛИЗАЦИИ:")
print("="*50)

print("\nСтатистика по методам:")
stats = results_init_df.groupby('method').agg({
    'inertia': ['mean', 'std'],
    'silhouette': ['mean', 'std'],
    'time': ['mean', 'std'],
    'ari': ['mean', 'std']
}).round(4)

print(stats)

# ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ
plt.figure(figsize=(12, 8))

# 1. Сравнение инерции
plt.subplot(2, 2, 1)
for method in init_methods:
    method_data = results_init_df[results_init_df['method'] == method]
    plt.scatter([method] * len(method_data), method_data['inertia'], alpha=0.6, label=method, s=80)
plt.ylabel('Инерция')
plt.title('Сравнение инерции')
plt.legend()

# 2. Сравнение силуэтного коэффициента
plt.subplot(2, 2, 2)
for method in init_methods:
    method_data = results_init_df[results_init_df['method'] == method]
    plt.scatter([method] * len(method_data), method_data['silhouette'], alpha=0.6, label=method, s=80)
plt.ylabel('Силуэтный коэффициент')
plt.title('Сравнение качества кластеризации')
plt.legend()

# 3. Сравнение ARI
plt.subplot(2, 2, 3)
for method in init_methods:
    method_data = results_init_df[results_init_df['method'] == method]
    plt.scatter([method] * len(method_data), method_data['ari'], alpha=0.6, label=method, s=80)
plt.ylabel('Adjusted Rand Index')
plt.title('Сравнение с реальными классами')
plt.legend()

# 4. Сравнение времени
plt.subplot(2, 2, 4)
for method in init_methods:
    method_data = results_init_df[results_init_df['method'] == method]
    plt.scatter([method] * len(method_data), method_data['time'], alpha=0.6, label=method, s=80)
plt.ylabel('Время (сек)')
plt.title('Сравнение времени выполнения')
plt.legend()

plt.tight_layout()
plt.show()

# ИСПРАВЛЕННЫЙ ВЫБОР ЛУЧШЕГО МЕТОДА
print(f"\nРЕКОМЕНДАЦИИ:")

# Способ 1: По среднему силуэтному коэффициенту
silhouette_means = results_init_df.groupby('method')['silhouette'].mean()
best_method_silhouette = silhouette_means.idxmax()
best_silhouette_value = silhouette_means.max()

print(f"Лучший метод по силуэтному коэффициенту: {best_method_silhouette} ({best_silhouette_value:.4f})")

# Способ 2: По среднему ARI
ari_means = results_init_df.groupby('method')['ari'].mean()
best_method_ari = ari_means.idxmax()
best_ari_value = ari_means.max()

print(f"Лучший метод по ARI: {best_method_ari} ({best_ari_value:.4f})")

# Способ 3: По стабильности (минимальное стандартное отклонение)
silhouette_std = results_init_df.groupby('method')['silhouette'].std()
best_method_stable = silhouette_std.idxmin()
best_std_value = silhouette_std.min()

print(f"Самый стабильный метод: {best_method_stable} (std: {best_std_value:.4f})")

# ФИНАЛЬНОЕ ОБУЧЕНИЕ С ЛУЧШИМ МЕТОДОМ
final_method = best_method_silhouette  # Используем лучший по силуэту

print(f"\nФИНАЛЬНОЕ ОБУЧЕНИЕ С МЕТОДОМ '{final_method}' НА ВСЕХ ДАННЫХ...")
start_time = time.time()
kmeans_final = KMeans(n_clusters=optimal_k, init=final_method, n_init=10, random_state=42)
final_labels = kmeans_final.fit_predict(X_scaled)
final_time = time.time() - start_time

final_silhouette = silhouette_score(X_scaled, final_labels)
final_ari = adjusted_rand_score(real_labels, final_labels)

print(f"Обучение завершено за {final_time:.2f} сек")
print(f"Финальный силуэтный коэффициент: {final_silhouette:.4f}")
print(f"Финальный ARI: {final_ari:.4f}")
print(f"Найдено кластеров: {len(np.unique(final_labels))}")

# Сохраняем финальные метки для последующего использования
kmeans_labels = final_labels
#%%
print("\nБЛОК 4: АНСАМБЛЕВАЯ КЛАСТЕРИЗАЦИЯ")

from scipy.stats import mode
def fast_ensemble_kmeans(X, n_clusters, n_models=5, subsample_ratio=0.3, random_state=42):
    # Быстрая ансамблевая кластеризация с подвыборкой
    np.random.seed(random_state)
    all_labels = []

    # Используем подвыборку для ускорения
    n_samples = X.shape[0]
    subsample_size = int(n_samples * subsample_ratio)
    indices = np.random.choice(n_samples, subsample_size, replace=False)
    X_subsample = X[indices]

    print(f"Используем подвыборку: {subsample_size} из {n_samples} образцов")

    for i in range(n_models):
        kmeans = KMeans(n_clusters=n_clusters, n_init=1,
                        init='random', random_state=random_state + i)
        labels = kmeans.fit_predict(X_subsample)
        all_labels.append(labels)

    # Создаем разреженную матрицу сходства только для подвыборки
    similarity_matrix = np.zeros((subsample_size, subsample_size))

    for labels in all_labels:
        for i in range(subsample_size):
            for j in range(i, subsample_size):  # Используем симметрию
                if labels[i] == labels[j]:
                    similarity_matrix[i, j] += 1
                    similarity_matrix[j, i] += 1

    similarity_matrix /= n_models

    # Кластеризуем подвыборку
    from sklearn.cluster import SpectralClustering
    spectral = SpectralClustering(n_clusters=n_clusters,
                                  affinity='precomputed',
                                  random_state=random_state)
    ensemble_labels_subsample = spectral.fit_predict(similarity_matrix)

    # Распространяем кластеры на все данные
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_subsample, ensemble_labels_subsample)
    ensemble_labels = knn.predict(X)

    return ensemble_labels, all_labels


def simple_ensemble_kmeans(X, n_clusters, n_models=5, random_state=42):

    # Упрощенная ансамблевая кластеризация через голосование
    np.random.seed(random_state)
    all_labels = []

    print(f"Запуск {n_models} моделей K-Means...")

    for i in range(n_models):
        kmeans = KMeans(n_clusters=n_clusters, n_init=1,
                        init='random', random_state=random_state + i)
        labels = kmeans.fit_predict(X)
        all_labels.append(labels)
        print(f"Модель {i + 1} завершена")

    # Простое усреднение меток через моду
    all_labels_array = np.array(all_labels).T  # Транспонируем для удобства
    ensemble_labels = np.apply_along_axis(
        lambda x: mode(x, keepdims=True)[0][0],
        axis=1,
        arr=all_labels_array
    )

    return ensemble_labels, all_labels


# Выбираем метод в зависимости от размера данных
if len(X_scaled) > 10000:
    print("Большой датасет, используем упрощенный метод...")
    start_time = time.time()
    ensemble_labels, individual_labels = simple_ensemble_kmeans(X_scaled, optimal_k, n_models=5)
    ensemble_time = time.time() - start_time
else:
    print("Используем быстрый метод с подвыборкой...")
    start_time = time.time()
    ensemble_labels, individual_labels = fast_ensemble_kmeans(X_scaled, optimal_k, n_models=5)
    ensemble_time = time.time() - start_time

# Оценка ансамбля
ensemble_silhouette = silhouette_score(X_scaled, ensemble_labels)
ensemble_ari = adjusted_rand_score(real_labels, ensemble_labels)

print(f"\nРезультаты ансамблевой кластеризации:")
print(f"Silhouette Score: {ensemble_silhouette:.4f}")
print(f"Adjusted Rand Index: {ensemble_ari:.4f}")
print(f"Время выполнения: {ensemble_time:.2f} сек")

# Сравнение с лучшей одиночной моделью
print(f"\nСравнение с лучшей одиночной моделью:")
print(f"Лучший силуэт одиночной: {results_init_df['silhouette'].max():.4f}")
print(f"Лучший ARI одиночной: {results_init_df['ari'].max():.4f}")
print(f"Улучшение силуэта: {ensemble_silhouette - results_init_df['silhouette'].max():.4f}")
print(f"Улучшение ARI: {ensemble_ari - results_init_df['ari'].max():.4f}")
#%%
print("\nБЛОК 5: DBSCAN — ПОДБОР ПАРАМЕТРА EPS")

def find_optimal_eps(X, k_values=[10, 15, 20, 30]):

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.ravel()

    eps_list = []

    for i, k in enumerate(k_values):
        neigh = NearestNeighbors(n_neighbors=k).fit(X)
        distances, _ = neigh.kneighbors(X)

        k_dist = np.sort(distances[:, -1])

        # сглаживаем кривую (устраняем шум)
        k_dist_smooth = pd.Series(k_dist).rolling(50, min_periods=1).mean()

        # ищем точку максимального скачка
        diffs = np.diff(k_dist_smooth)
        elbow = np.argmax(diffs)
        eps = k_dist_smooth[elbow]

        eps_list.append(eps)

        # --- визуализация ---
        ax = axes[i]
        ax.plot(k_dist, label="k-dist", alpha=0.6)
        ax.plot(k_dist_smooth, label="Сглаженный", linewidth=2)
        ax.axhline(eps, color="red", linestyle="--", label=f"eps={eps:.3f}")
        ax.axvline(elbow, color="green", linestyle="--", label="локоть")
        ax.set_title(f"k = {k}")
        ax.legend()
        ax.grid(alpha=0.3)

    plt.tight_layout()
    plt.show()

    # усредняем, но не допускаем огромных значений
    final_eps = float(np.median(eps_list))

    # safety-clip: eps обычно <= 1.2 для нормализованных данных
    final_eps = min(final_eps, 1.5)

    return final_eps


optimal_eps = find_optimal_eps(X_scaled)
print(f"\nРекомендуемое значение eps: {optimal_eps:.3f}")

#%%
print("\n=== БЛОК 6: DBSCAN (оптимизированный) ===")

# Eps
optimal_eps = globals().get("optimal_eps", 0.5)
print(f"Используем eps = {optimal_eps:.3f}")
print(f"Данные: {X_scaled.shape}")

# 1. Подвыборка
MAX_SAMPLES = 20_000
if X_scaled.shape[0] > MAX_SAMPLES:
    idx = np.random.choice(len(X_scaled), MAX_SAMPLES, replace=False)
    X_db = X_scaled[idx]
    y_db = real_labels[idx]
    sampled = True
else:
    X_db = X_scaled
    y_db = real_labels
    sampled = False

print(f"Использовано для DBSCAN: {len(X_db):,} (подвыборка = {sampled})")

#  2. Быстрый подбор min_samples
print("\n1) Подбор min_samples...")

min_samples_list = [3, 5, 7, 10]
sample_n = min(2000, len(X_db))

X_small = X_db[:sample_n]
y_small = y_db[:sample_n]

def test_min_samples(ms):
    try:
        db = DBSCAN(eps=optimal_eps, min_samples=ms).fit(X_small)
        labels = db.labels_

        clusters = len(np.unique(labels[labels != -1]))
        noise = np.sum(labels == -1) / len(labels)

        if clusters < 2:
            return (-1, -1, clusters, noise)

        sil = silhouette_score(X_small, labels)
        ari = adjusted_rand_score(y_small, labels)
        return (sil, ari, clusters, noise)

    except:
        return (-1, -1, -1, -1)


results = []
for ms in min_samples_list:
    sil, ari, clusters, noise = test_min_samples(ms)
    print(f"  min_samples={ms}: clusters={clusters}, noise={noise:.3f}, silhouette={sil:.3f}")
    results.append((ms, sil, ari, clusters, noise))

# выбираем лучший по silhouette
best_ms = max(results, key=lambda x: x[1])[0]
print(f"\nЛучший min_samples = {best_ms}")

#  3. Финальный запуск
print("\n2) Запуск DBSCAN...")
start = time.time()

db_final = DBSCAN(eps=optimal_eps, min_samples=best_ms).fit(X_db)
labels_final = db_final.labels_

elapsed = time.time() - start

clusters = len(np.unique(labels_final[labels_final != -1]))
noise = np.sum(labels_final == -1)

if clusters >= 2:
    sil = silhouette_score(X_db, labels_final)
    ari = adjusted_rand_score(y_db, labels_final)
else:
    sil, ari = -1, -1

print(f"✓ Готово за {elapsed:.2f} сек")
print(f"  Кластеры: {clusters}")
print(f"  Шум: {noise} ({noise/len(labels_final):.1%})")
print(f"  Silhouette: {sil:.4f}")
print(f"  ARI: {ari:.4f}")

best_dbscan = {
    "eps": optimal_eps,
    "min_samples": best_ms,
    "labels": labels_final,
    "clusters": clusters,
    "noise": noise,
    "silhouette": sil,
    "ari": ari,
    "sample_size": len(X_db),
    "is_sampled": sampled
}

#  4. Визуализация
print("\n3) Визуализация...")

if X_db.shape[1] > 2:
    X_vis = PCA(n_components=2).fit_transform(X_db)
else:
    X_vis = X_db

plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.scatter(X_vis[:,0], X_vis[:,1], c=labels_final, cmap='tab10', s=12, alpha=0.6)
plt.title(f"DBSCAN ({clusters} кл., шум={noise})")

plt.subplot(1,2,2)
plt.scatter(X_vis[:,0], X_vis[:,1], c=y_db, cmap='tab10', s=12, alpha=0.6)
plt.title("Истинные классы")

plt.tight_layout()
plt.show()

#  5. Итог
print("\n4) Итоговая информация:")
print(f"Исходный размер: {len(X_scaled):,}")
print(f"Использовано в DBSCAN: {len(X_db):,} ({len(X_db)/len(X_scaled)*100:.1f}%)")

print("\n=== БЛОК 6 завершён ===")

#%%
print("\n БЛОК 7: ИЕРАРХИЧЕСКАЯ КЛАСТЕРИЗАЦИЯ - ДЕНДРОГРАММЫ ")

from scipy.cluster.hierarchy import dendrogram, linkage
import numpy as np
import matplotlib.pyplot as plt

# 1. Случайная подвыборка

sample_size = min(1500, X_scaled.shape[0])
idx = np.random.choice(len(X_scaled), sample_size, replace=False)
X_sample = X_scaled[idx].astype(float)

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
axes = axes.ravel()

linkage_methods = ['ward', 'complete', 'average', 'single']

# 2. Построение дендрограмм

Z_matrices = {}

for i, method in enumerate(linkage_methods):
    ax = axes[i]
    try:
        Z = linkage(X_sample, method=method)
        Z_matrices[method] = Z

        dendrogram(
            Z,
            ax=ax,
            truncate_mode='lastp',
            p=30,                 # больше информации
            show_leaf_counts=True,
            leaf_rotation=90
        )

        ax.set_title(f"Дендрограмма ({method} linkage)")
        ax.set_ylabel("Расстояние")

    except Exception as e:
        ax.text(0.5, 0.5, f"Ошибка:\n{str(e)}",
                transform=ax.transAxes, ha='center', va='center')

plt.tight_layout()
plt.show()

# 3. Поиск оптимального числа кластеров

def find_optimal_clusters_dendrogram(Z, max_k=12):

    merge_heights = Z[-max_k:, 2]
    diffs = np.diff(merge_heights)

    if len(diffs) == 0:
        return 2

    best_gap = np.argmax(diffs)
    k = max_k - best_gap
    return max(k, 2)

# 4. Используем Ward для выбора k

if "ward" in Z_matrices:
    Z_ward = Z_matrices["ward"]
    optimal_k_hierarchical = find_optimal_clusters_dendrogram(Z_ward, max_k=12)
    print(f"Оптимальное число кластеров (по Ward дендрограмме): {optimal_k_hierarchical}")

else:
    optimal_k_hierarchical = optimal_k
    print(f"Ward недоступен — используем k из KMeans: {optimal_k_hierarchical}")

#%%
print("\n БЛОК 8: ИЕРАРХИЧЕСКАЯ - МЕТРИКИ РАССТОЯНИЯ (ОПТИМИЗИРОВАНО)")

from sklearn.cluster import AgglomerativeClustering

# 1. Быстрая репрезентативная подвыборка

subsample = min(2000, X_scaled.shape[0])
idx = np.random.choice(X_scaled.shape[0], subsample, replace=False)
X_small = X_scaled[idx]
y_small = real_labels[idx]

print(f"Используем подвыборку: {subsample} образцов")

# 2. Метрики и разрешённые линкейджи

metrics = ['euclidean', 'manhattan', 'cosine']
results = []

for metric in metrics:

    # Ward - только euclidean
    if metric == 'euclidean':
        linkages = ['ward', 'average']
    else:
        linkages = ['average']

    for link in linkages:
        try:
            start = time.time()

            hc = AgglomerativeClustering(
                n_clusters=optimal_k_hierarchical,
                metric=metric,
                linkage=link
            )

            labels = hc.fit_predict(X_small)
            elapsed = time.time() - start

            if len(np.unique(labels)) >= 2:
                sil = silhouette_score(X_small, labels)
                ari = adjusted_rand_score(y_small, labels)
            else:
                sil, ari = -1, -1

            results.append({
                'metric': metric,
                'linkage': link,
                'silhouette': sil,
                'ari': ari,
                'time': elapsed
            })

        except Exception as e:
            results.append({
                'metric': metric,
                'linkage': link,
                'silhouette': -1,
                'ari': -1,
                'time': -1
            })
            print(f"Ошибка при metric={metric}, linkage={link}: {e}")

# 3. Вывод результатов

df_hier = pd.DataFrame(results)
print("\nСравнение метрик расстояния (по подвыборке):")
print(df_hier.round(4))

#%%
print("\n БЛОК 9: АНАЛИЗ МАСШТАБИРУЕМОСТИ (ОПТИМИЗИРОВАНО)")

sample_sizes = [1000, 2500, 5000, 10000]
scalability_results = []

# Ограничение для иерархической кластеризации
HIERARCHY_LIMIT = 10001   # больше — слишком медленно

for size in sample_sizes:
    if size > len(X_scaled):
        continue

    print(f"\nТестируем размер: {size}")
    X_subset = X_scaled[:size]

    # 1) K-MEANS (O(n * k * d)) → быстрый
    start = time.time()
    KMeans(n_clusters=optimal_k, n_init=5, random_state=0).fit_predict(X_subset)
    kmeans_time = time.time() - start

    # 2) DBSCAN (сильно зависит от eps)

    start = time.time()
    DBSCAN(
        eps=best_dbscan['eps'],
        min_samples=best_dbscan['min_samples']
    ).fit_predict(X_subset)
    dbscan_time = time.time() - start

    # 3) ИЕРАРХИЧЕСКАЯ КЛАСТЕРИЗАЦИЯ

    if size <= HIERARCHY_LIMIT:
        start = time.time()
        AgglomerativeClustering(
            n_clusters=optimal_k_hierarchical,
            metric="euclidean",
            linkage="ward"
        ).fit_predict(X_subset)
        hierarchical_time = time.time() - start
    else:
        hierarchical_time = None  # слишком дорого

    scalability_results.append({
        'sample_size': size,
        'kmeans_time': kmeans_time,
        'dbscan_time': dbscan_time,
        'hierarchical_time': hierarchical_time
    })

scalability_df = pd.DataFrame(scalability_results)

# ВИЗУАЛИЗАЦИЯ

plt.figure(figsize=(10, 6))
plt.plot(
    scalability_df['sample_size'],
    scalability_df['kmeans_time'],
    'o-', label='K-Means', linewidth=2, markersize=8
)
plt.plot(
    scalability_df['sample_size'],
    scalability_df['dbscan_time'],
    's-', label='DBSCAN', linewidth=2, markersize=8
)

# Рисуем иерархическую только там, где есть данные
mask = scalability_df['hierarchical_time'].notna()
plt.plot(
    scalability_df['sample_size'][mask],
    scalability_df['hierarchical_time'][mask],
    '^-', label='Hierarchical', linewidth=2, markersize=8
)

plt.xlabel('Размер выборки')
plt.ylabel('Время выполнения (сек)')
plt.title('Масштабируемость алгоритмов кластеризации')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print("\nАнализ масштабируемости:")
print(scalability_df.round(4))

#%%
print("\nБЛОК 10: ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ (оптимизированная)")

VISUALIZE_N = 5000  # подвыборка для визуализации
idx = np.random.choice(X_scaled.shape[0], VISUALIZE_N, replace=False)

X_pca_vis = X_pca[idx]
real_vis = real_labels[idx]

# K-Means на полном наборе
kmeans_model = KMeans(n_clusters=optimal_k, n_init=10, random_state=42)
kmeans_labels = kmeans_model.fit_predict(X_scaled)[idx]

# DBSCAN на подвыборке (чтобы не лезть в память)
dbscan_model = DBSCAN(eps=best_dbscan['eps'], min_samples=best_dbscan['min_samples'])
X_dbscan_vis = X_scaled[idx]  # используем подвыборку
dbscan_labels = dbscan_model.fit_predict(X_dbscan_vis)

# Hierarchical на подвыборке
hierarchical_model = AgglomerativeClustering(n_clusters=optimal_k_hierarchical)
hierarchical_labels = hierarchical_model.fit_predict(X_scaled[idx])

# Визуализация
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

plots = [
    ('Реальные классы', real_vis),
    (f'K-Means (k={optimal_k})', kmeans_labels),
    (f'DBSCAN (clusters: {len(np.unique(dbscan_labels[dbscan_labels != -1]))})', dbscan_labels),
    (f'Hierarchical (k={optimal_k_hierarchical})', hierarchical_labels)
]

for ax, (title, labels) in zip(axes.ravel(), plots):
    scatter = ax.scatter(X_pca_vis[:, 0], X_pca_vis[:, 1], c=labels,
                         cmap='tab10', alpha=0.6, s=10)
    ax.set_title(title)
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
    plt.colorbar(scatter, ax=ax)

plt.tight_layout()
plt.show()


#%%
print("\nБЛОК 11: ОЦЕНКА УСТОЙЧИВОСТИ (оптимизированная для больших данных)")

from sklearn.metrics import adjusted_mutual_info_score

def cluster_stability_analysis(X, algorithm, n_subsamples=3, subsample_ratio=0.3, max_subset=5000, random_state=42):
    np.random.seed(random_state)
    n_samples = X.shape[0]
    subsample_size = int(n_samples * subsample_ratio)
    all_labels = []

    for i in range(n_subsamples):
        indices = np.random.choice(n_samples, subsample_size, replace=False)
        X_subsample = X[indices]

        # Ограничение размера для DBSCAN и Hierarchical
        if algorithm in ['DBSCAN', 'Hierarchical'] and X_subsample.shape[0] > max_subset:
            X_subsample = X_subsample[:max_subset]
            indices = indices[:max_subset]

        # Инициализация модели
        if algorithm == 'K-Means':
            model = KMeans(n_clusters=optimal_k, n_init=5, random_state=random_state+i)
        elif algorithm == 'DBSCAN':
            model = DBSCAN(eps=best_dbscan['eps'], min_samples=best_dbscan['min_samples'])
        else:  # Hierarchical
            model = AgglomerativeClustering(n_clusters=optimal_k_hierarchical)

        labels = model.fit_predict(X_subsample)
        if len(np.unique(labels)) >= 2:
            all_labels.append((indices, labels))

    # Попарное сравнение с AMI
    scores = []
    for i in range(len(all_labels)):
        for j in range(i+1, len(all_labels)):
            idx_i, lab_i = all_labels[i]
            idx_j, lab_j = all_labels[j]
            common_idx = np.intersect1d(idx_i, idx_j)
            if len(common_idx) == 0:
                continue
            map_i = [np.where(idx_i == k)[0][0] for k in common_idx]
            map_j = [np.where(idx_j == k)[0][0] for k in common_idx]
            lab_i_common = lab_i[map_i]
            lab_j_common = lab_j[map_j]
            score = adjusted_mutual_info_score(lab_i_common, lab_j_common)
            scores.append(score)

    return np.mean(scores) if scores else 0

# Запуск анализа
stability_results = []
for algo in ['K-Means', 'DBSCAN', 'Hierarchical']:
    print(f"Анализ устойчивости для {algo}...")
    stability = cluster_stability_analysis(X_scaled, algo, n_subsamples=3, subsample_ratio=0.3, max_subset=5000)
    stability_results.append({'Algorithm': algo, 'Stability_Score': stability})

stability_df = pd.DataFrame(stability_results)
print("\nРезультаты анализа устойчивости:")
print(stability_df.round(4))
